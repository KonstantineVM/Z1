{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction Models Using Decomposed Components\n",
    "\n",
    "This notebook demonstrates how to build prediction models using the decomposed components from Z1 time series data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ML imports\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import Lasso, Ridge, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "\n",
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from src.analysis.feature_engineering import FeatureEngineer\n",
    "from src.models.prediction_models import TimeSeriesPredictor\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Decomposed Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decomposed components from previous notebook\n",
    "data_dir = Path('../data/decomposed')\n",
    "\n",
    "components = {}\n",
    "for comp_file in data_dir.glob('*_components.parquet'):\n",
    "    comp_name = comp_file.stem.replace('_components', '')\n",
    "    components[comp_name] = pd.read_parquet(comp_file)\n",
    "    print(f\"Loaded {comp_name}: {components[comp_name].shape}\")\n",
    "\n",
    "# Load metadata\n",
    "with open(data_dir / 'decomposition_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\nTotal series: {len(metadata['series'])}\")\n",
    "print(f\"Components: {metadata['components']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FeatureEngineer()\n",
    "\n",
    "# Select target variable for prediction\n",
    "target_series = metadata['series'][0]  # First series as example\n",
    "print(f\"Target series for prediction: {target_series}\")\n",
    "\n",
    "# Extract cycle component as primary feature\n",
    "if 'cycle' in components:\n",
    "    cycle_df = components['cycle'].iloc[30:]  # Skip initial observations\n",
    "    print(f\"\\nCycle component shape: {cycle_df.shape}\")\n",
    "else:\n",
    "    print(\"No cycle component found. Using trend instead.\")\n",
    "    cycle_df = components['trend'].iloc[30:]\n",
    "\n",
    "# Create lagged features\n",
    "max_lags = 8\n",
    "lagged_features = []\n",
    "\n",
    "for lag in range(1, max_lags + 1):\n",
    "    lagged_df = cycle_df.shift(lag).add_suffix(f'_lag{lag}')\n",
    "    lagged_features.append(lagged_df)\n",
    "\n",
    "# Combine all features\n",
    "feature_df = pd.concat([cycle_df] + lagged_features, axis=1)\n",
    "feature_df = feature_df.dropna()\n",
    "\n",
    "print(f\"\\nTotal features created: {feature_df.shape[1]}\")\n",
    "print(f\"Feature matrix shape: {feature_df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prepare Target Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the target series cycle component as the target\n",
    "if target_series in cycle_df.columns:\n",
    "    y = cycle_df[target_series].loc[feature_df.index]\n",
    "else:\n",
    "    # If target not in cycle, use the first available series\n",
    "    target_series = cycle_df.columns[0]\n",
    "    y = cycle_df[target_series].loc[feature_df.index]\n",
    "    print(f\"Updated target series: {target_series}\")\n",
    "\n",
    "# Remove target from features\n",
    "X = feature_df.drop(columns=[col for col in feature_df.columns if target_series in col])\n",
    "\n",
    "print(f\"\\nFeatures shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "\n",
    "# Visualize target variable\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y.index, y.values)\n",
    "plt.title(f'Target Variable: {target_series} (Cycle Component)')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time series split - maintain temporal order\n",
    "split_ratio = 0.8\n",
    "split_point = int(len(X) * split_ratio)\n",
    "\n",
    "X_train, X_test = X.iloc[:split_point], X.iloc[split_point:]\n",
    "y_train, y_test = y.iloc[:split_point], y.iloc[split_point:]\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"\\nTraining period: {X_train.index.min()} to {X_train.index.max()}\")\n",
    "print(f\"Test period: {X_test.index.min()} to {X_test.index.max()}\")\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. LASSO Regression with Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LASSO model\n",
    "lasso = Lasso(alpha=0.003, max_iter=10000)\n",
    "lasso.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lasso = lasso.predict(X_test_scaled)\n",
    "\n",
    "# Evaluate performance\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "mae_lasso = mean_absolute_error(y_test, y_pred_lasso)\n",
    "r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "\n",
    "print(\"LASSO Model Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MSE: {mse_lasso:.6f}\")\n",
    "print(f\"MAE: {mae_lasso:.6f}\")\n",
    "print(f\"RÂ²: {r2_lasso:.4f}\")\n",
    "\n",
    "# Feature importance from LASSO coefficients\n",
    "feature_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Coefficient': lasso.coef_\n",
    "})\n",
    "feature_importance['Abs_Coefficient'] = np.abs(feature_importance['Coefficient'])\n",
    "feature_importance = feature_importance.sort_values('Abs_Coefficient', ascending=False)\n",
    "\n",
    "# Display top features\n",
    "print(\"\\nTop 20 Important Features:\")\n",
    "print(feature_importance.head(20)[['Feature', 'Coefficient']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize LASSO predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test.values, 'b-', label='Actual', linewidth=2)\n",
    "plt.plot(y_test.index, y_pred_lasso, 'r--', label='LASSO Predicted', linewidth=2)\n",
    "plt.title(f'LASSO Predictions vs Actual - {target_series}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Residual plot\n",
    "residuals_lasso = y_test - y_pred_lasso\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(y_test.index, residuals_lasso, 'g-', alpha=0.7)\n",
    "plt.axhline(y=0, color='black', linestyle='--')\n",
    "plt.title('LASSO Prediction Residuals')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Residual')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure XGBoost parameters\n",
    "xgb_params = {\n",
    "    'lambda': 0.05,\n",
    "    'alpha': 0.02,\n",
    "    'gamma': 0.000001,\n",
    "    'colsample_bytree': 0.43,\n",
    "    'subsample': 0.99,\n",
    "    'learning_rate': 0.05,\n",
    "    'n_estimators': 300,\n",
    "    'max_depth': 4,\n",
    "    'min_child_weight': 3,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "xgb_model.fit(X_train, y_train, \n",
    "              eval_set=[(X_test, y_test)],\n",
    "              early_stopping_rounds=50,\n",
    "              verbose=False)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_xgb = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "print(\"XGBoost Model Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MSE: {mse_xgb:.6f}\")\n",
    "print(f\"MAE: {mae_xgb:.6f}\")\n",
    "print(f\"RÂ²: {r2_xgb:.4f}\")\n",
    "print(f\"\\nBest iteration: {xgb_model.best_iteration}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance from XGBoost\n",
    "xgb_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': xgb_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = xgb_importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "plt.xlabel('Feature Importance')\n",
    "plt.title('Top 20 Features - XGBoost')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=500,\n",
    "    max_depth=5,\n",
    "    min_samples_split=10,\n",
    "    min_samples_leaf=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate performance\n",
    "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "r2_rf = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Model Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"MSE: {mse_rf:.6f}\")\n",
    "print(f\"MAE: {mae_rf:.6f}\")\n",
    "print(f\"RÂ²: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models\n",
    "model_comparison = pd.DataFrame({\n",
    "    'Model': ['LASSO', 'XGBoost', 'Random Forest'],\n",
    "    'MSE': [mse_lasso, mse_xgb, mse_rf],\n",
    "    'MAE': [mae_lasso, mae_xgb, mae_rf],\n",
    "    'RÂ²': [r2_lasso, r2_xgb, r2_rf]\n",
    "})\n",
    "\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(\"=\" * 50)\n",
    "print(model_comparison.round(4))\n",
    "\n",
    "# Visualize model predictions\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "\n",
    "models = [\n",
    "    ('LASSO', y_pred_lasso, 'red'),\n",
    "    ('XGBoost', y_pred_xgb, 'green'),\n",
    "    ('Random Forest', y_pred_rf, 'orange')\n",
    "]\n",
    "\n",
    "for idx, (name, pred, color) in enumerate(models):\n",
    "    axes[idx].plot(y_test.index, y_test.values, 'b-', label='Actual', linewidth=2, alpha=0.7)\n",
    "    axes[idx].plot(y_test.index, pred, color=color, linestyle='--', label=f'{name} Predicted', linewidth=2)\n",
    "    axes[idx].set_title(f'{name} Model Predictions')\n",
    "    axes[idx].set_ylabel('Value')\n",
    "    axes[idx].legend()\n",
    "    axes[idx].grid(True, alpha=0.3)\n",
    "\n",
    "axes[-1].set_xlabel('Date')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Ensemble Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble prediction (weighted average)\n",
    "# Weight by RÂ² scores\n",
    "weights = np.array([r2_lasso, r2_xgb, r2_rf])\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "y_pred_ensemble = (\n",
    "    weights[0] * y_pred_lasso +\n",
    "    weights[1] * y_pred_xgb +\n",
    "    weights[2] * y_pred_rf\n",
    ")\n",
    "\n",
    "# Evaluate ensemble\n",
    "mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)\n",
    "mae_ensemble = mean_absolute_error(y_test, y_pred_ensemble)\n",
    "r2_ensemble = r2_score(y_test, y_pred_ensemble)\n",
    "\n",
    "print(\"Ensemble Model Performance:\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Weights: LASSO={weights[0]:.3f}, XGBoost={weights[1]:.3f}, RF={weights[2]:.3f}\")\n",
    "print(f\"MSE: {mse_ensemble:.6f}\")\n",
    "print(f\"MAE: {mae_ensemble:.6f}\")\n",
    "print(f\"RÂ²: {r2_ensemble:.4f}\")\n",
    "\n",
    "# Plot ensemble predictions\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test.index, y_test.values, 'b-', label='Actual', linewidth=2)\n",
    "plt.plot(y_test.index, y_pred_ensemble, 'purple', linestyle='--', label='Ensemble Predicted', linewidth=2)\n",
    "plt.title(f'Ensemble Model Predictions - {target_series}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Time Series Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform time series cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "cv_scores = {'LASSO': [], 'XGBoost': [], 'Random Forest': []}\n",
    "\n",
    "print(\"Time Series Cross-Validation:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    X_cv_train, X_cv_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_cv_train, y_cv_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Scale data\n",
    "    scaler_cv = StandardScaler()\n",
    "    X_cv_train_scaled = scaler_cv.fit_transform(X_cv_train)\n",
    "    X_cv_val_scaled = scaler_cv.transform(X_cv_val)\n",
    "    \n",
    "    # LASSO\n",
    "    lasso_cv = Lasso(alpha=0.003, max_iter=10000)\n",
    "    lasso_cv.fit(X_cv_train_scaled, y_cv_train)\n",
    "    cv_scores['LASSO'].append(r2_score(y_cv_val, lasso_cv.predict(X_cv_val_scaled)))\n",
    "    \n",
    "    # XGBoost\n",
    "    xgb_cv = xgb.XGBRegressor(**xgb_params)\n",
    "    xgb_cv.fit(X_cv_train, y_cv_train, verbose=False)\n",
    "    cv_scores['XGBoost'].append(r2_score(y_cv_val, xgb_cv.predict(X_cv_val)))\n",
    "    \n",
    "    # Random Forest\n",
    "    rf_cv = RandomForestRegressor(n_estimators=500, max_depth=5, random_state=42, n_jobs=-1)\n",
    "    rf_cv.fit(X_cv_train, y_cv_train)\n",
    "    cv_scores['Random Forest'].append(r2_score(y_cv_val, rf_cv.predict(X_cv_val)))\n",
    "    \n",
    "    print(f\"Fold {fold+1}: LASSO={cv_scores['LASSO'][-1]:.3f}, \"\n",
    "          f\"XGBoost={cv_scores['XGBoost'][-1]:.3f}, \"\n",
    "          f\"RF={cv_scores['Random Forest'][-1]:.3f}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nCross-Validation Summary (RÂ² scores):\")\n",
    "for model, scores in cv_scores.items():\n",
    "    print(f\"{model}: Mean={np.mean(scores):.3f}, Std={np.std(scores):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Feature Analysis and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze most important features across models\n",
    "# Get top features from each model\n",
    "top_n = 10\n",
    "\n",
    "# LASSO top features\n",
    "lasso_top = feature_importance.head(top_n)['Feature'].tolist()\n",
    "\n",
    "# XGBoost top features  \n",
    "xgb_top = xgb_importance.head(top_n)['Feature'].tolist()\n",
    "\n",
    "# Random Forest top features\n",
    "rf_importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': rf_model.feature_importances_\n",
    "}).sort_values('Importance', ascending=False)\n",
    "rf_top = rf_importance.head(top_n)['Feature'].tolist()\n",
    "\n",
    "# Find common important features\n",
    "common_features = set(lasso_top) & set(xgb_top) & set(rf_top)\n",
    "print(f\"Common top features across all models: {len(common_features)}\")\n",
    "for feat in common_features:\n",
    "    print(f\"  - {feat}\")\n",
    "\n",
    "# Create feature importance comparison\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# LASSO\n",
    "axes[0].barh(range(top_n), feature_importance.head(top_n)['Abs_Coefficient'])\n",
    "axes[0].set_yticks(range(top_n))\n",
    "axes[0].set_yticklabels(feature_importance.head(top_n)['Feature'])\n",
    "axes[0].set_title('LASSO - Top Features')\n",
    "axes[0].invert_yaxis()\n",
    "\n",
    "# XGBoost\n",
    "axes[1].barh(range(top_n), xgb_importance.head(top_n)['Importance'])\n",
    "axes[1].set_yticks(range(top_n))\n",
    "axes[1].set_yticklabels(xgb_importance.head(top_n)['Feature'])\n",
    "axes[1].set_title('XGBoost - Top Features')\n",
    "axes[1].invert_yaxis()\n",
    "\n",
    "# Random Forest\n",
    "axes[2].barh(range(top_n), rf_importance.head(top_n)['Importance'])\n",
    "axes[2].set_yticks(range(top_n))\n",
    "axes[2].set_yticklabels(rf_importance.head(top_n)['Feature'])\n",
    "axes[2].set_title('Random Forest - Top Features')\n",
    "axes[2].invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Out-of-Sample Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate out-of-sample forecasts\n",
    "# Use the best performing model (based on RÂ²)\n",
    "best_model_name = model_comparison.loc[model_comparison['RÂ²'].idxmax(), 'Model']\n",
    "print(f\"Best model for forecasting: {best_model_name}\")\n",
    "\n",
    "# Retrain on full dataset\n",
    "X_full = pd.concat([X_train, X_test])\n",
    "y_full = pd.concat([y_train, y_test])\n",
    "\n",
    "if best_model_name == 'XGBoost':\n",
    "    final_model = xgb.XGBRegressor(**xgb_params)\n",
    "    final_model.fit(X_full, y_full, verbose=False)\n",
    "elif best_model_name == 'Random Forest':\n",
    "    final_model = RandomForestRegressor(n_estimators=500, max_depth=5, random_state=42, n_jobs=-1)\n",
    "    final_model.fit(X_full, y_full)\n",
    "else:  # LASSO\n",
    "    X_full_scaled = scaler.fit_transform(X_full)\n",
    "    final_model = Lasso(alpha=0.003, max_iter=10000)\n",
    "    final_model.fit(X_full_scaled, y_full)\n",
    "\n",
    "print(\"\\nModel retrained on full dataset for forecasting\")\n",
    "\n",
    "# Note: For actual forecasting, you would need to:\n",
    "# 1. Generate future feature values (using lagged predictions)\n",
    "# 2. Make recursive predictions\n",
    "# 3. Transform predictions back to original scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Models and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save trained models and results\n",
    "import joblib\n",
    "\n",
    "output_dir = Path('../models')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "joblib.dump(lasso, output_dir / 'lasso_model.pkl')\n",
    "joblib.dump(xgb_model, output_dir / 'xgboost_model.pkl')\n",
    "joblib.dump(rf_model, output_dir / 'random_forest_model.pkl')\n",
    "joblib.dump(scaler, output_dir / 'feature_scaler.pkl')\n",
    "\n",
    "# Save predictions\n",
    "predictions_df = pd.DataFrame({\n",
    "    'actual': y_test,\n",
    "    'lasso': y_pred_lasso,\n",
    "    'xgboost': y_pred_xgb,\n",
    "    'random_forest': y_pred_rf,\n",
    "    'ensemble': y_pred_ensemble\n",
    "}, index=y_test.index)\n",
    "\n",
    "predictions_df.to_parquet(output_dir / 'predictions.parquet')\n",
    "\n",
    "# Save model performance\n",
    "model_comparison.to_csv(output_dir / 'model_performance.csv', index=False)\n",
    "\n",
    "# Save feature importance\n",
    "feature_importance.to_csv(output_dir / 'feature_importance_lasso.csv', index=False)\n",
    "xgb_importance.to_csv(output_dir / 'feature_importance_xgboost.csv', index=False)\n",
    "rf_importance.to_csv(output_dir / 'feature_importance_rf.csv', index=False)\n",
    "\n",
    "print(\"\\nâ Models and results saved successfully!\")\n",
    "print(f\"\\nSaved to: {output_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}