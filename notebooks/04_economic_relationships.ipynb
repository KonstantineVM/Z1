{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Economic Relationships Analysis\n",
    "\n",
    "This notebook explores economic relationships between different Z1 series using the decomposed components, analyzing cross-series dynamics, lead-lag relationships, and sector flows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical imports\n",
    "from scipy import stats\n",
    "from statsmodels.tsa.stattools import grangercausalitytests, ccf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "# Project imports\n",
    "import sys\n",
    "sys.path.append('..')  # Add parent directory to path\n",
    "\n",
    "from src.data.cached_fed_data_loader import CachedFedDataLoader\n",
    "from src.analysis.economic_analysis import EconomicAnalysis\n",
    "from src.visualization.economic_plots import EconomicVisualizer\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data and Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load decomposed components\n",
    "data_dir = Path('../data/decomposed')\n",
    "components = {}\n",
    "\n",
    "for comp_file in data_dir.glob('*_components.parquet'):\n",
    "    comp_name = comp_file.stem.replace('_components', '')\n",
    "    components[comp_name] = pd.read_parquet(comp_file)\n",
    "    print(f\"Loaded {comp_name}: {components[comp_name].shape}\")\n",
    "\n",
    "# Load original Z1 data for reference\n",
    "loader = CachedFedDataLoader(\n",
    "    base_directory=\"../data/fed_data\",\n",
    "    cache_directory=\"../data/cache\"\n",
    ")\n",
    "z1_data = loader.load_single_source('Z1')\n",
    "\n",
    "# Load metadata\n",
    "with open(data_dir / 'decomposition_metadata.json', 'r') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "print(f\"\\nAnalyzing {len(metadata['series'])} series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Identify Economic Series Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorize series by economic sector/type\n",
    "# This is based on Z1 series naming conventions\n",
    "series_categories = {\n",
    "    'household': [],\n",
    "    'corporate': [],\n",
    "    'financial': [],\n",
    "    'government': [],\n",
    "    'foreign': [],\n",
    "    'monetary': []\n",
    "}\n",
    "\n",
    "# Simple categorization based on series codes\n",
    "for series in metadata['series']:\n",
    "    if 'FL15' in series or 'FL01' in series:  # Household codes\n",
    "        series_categories['household'].append(series)\n",
    "    elif 'FL10' in series or 'FL11' in series:  # Nonfinancial corporate\n",
    "        series_categories['corporate'].append(series)\n",
    "    elif 'FL70' in series or 'FL71' in series:  # Financial sector\n",
    "        series_categories['financial'].append(series)\n",
    "    elif 'FL31' in series or 'FL21' in series:  # Government\n",
    "        series_categories['government'].append(series)\n",
    "    elif 'FL26' in series:  # Rest of world\n",
    "        series_categories['foreign'].append(series)\n",
    "    elif 'FL71' in series or 'FL72' in series:  # Monetary authority\n",
    "        series_categories['monetary'].append(series)\n",
    "\n",
    "# Display categorization\n",
    "print(\"Series by Category:\")\n",
    "print(\"=\" * 50)\n",
    "for category, series_list in series_categories.items():\n",
    "    if series_list:\n",
    "        print(f\"{category.capitalize()}: {len(series_list)} series\")\n",
    "        print(f\"  Examples: {series_list[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Cross-Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select representative series from different categories\n",
    "selected_pairs = []\n",
    "for cat1 in ['household', 'corporate', 'financial']:\n",
    "    for cat2 in ['household', 'corporate', 'financial']:\n",
    "        if cat1 != cat2 and series_categories[cat1] and series_categories[cat2]:\n",
    "            selected_pairs.append((\n",
    "                series_categories[cat1][0],\n",
    "                series_categories[cat2][0],\n",
    "                f\"{cat1}-{cat2}\"\n",
    "            ))\n",
    "\n",
    "# Compute cross-correlations for cycle components\n",
    "if 'cycle' in components:\n",
    "    cycle_df = components['cycle']\n",
    "    \n",
    "    fig, axes = plt.subplots(len(selected_pairs), 1, figsize=(12, 4*len(selected_pairs)))\n",
    "    if len(selected_pairs) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, (series1, series2, label) in enumerate(selected_pairs):\n",
    "        if series1 in cycle_df.columns and series2 in cycle_df.columns:\n",
    "            # Get common time period\n",
    "            data1 = cycle_df[series1].dropna()\n",
    "            data2 = cycle_df[series2].dropna()\n",
    "            common_idx = data1.index.intersection(data2.index)\n",
    "            \n",
    "            if len(common_idx) > 50:\n",
    "                # Compute cross-correlation\n",
    "                max_lags = 20\n",
    "                correlations = []\n",
    "                lags = range(-max_lags, max_lags + 1)\n",
    "                \n",
    "                for lag in lags:\n",
    "                    if lag < 0:\n",
    "                        corr = data1.iloc[:lag].corr(data2.iloc[-lag:])\n",
    "                    elif lag > 0:\n",
    "                        corr = data1.iloc[lag:].corr(data2.iloc[:-lag])\n",
    "                    else:\n",
    "                        corr = data1.corr(data2)\n",
    "                    correlations.append(corr)\n",
    "                \n",
    "                # Plot\n",
    "                axes[idx].bar(lags, correlations, alpha=0.7)\n",
    "                axes[idx].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "                axes[idx].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "                axes[idx].set_title(f'Cross-Correlation: {label}')\n",
    "                axes[idx].set_xlabel('Lag (quarters)')\n",
    "                axes[idx].set_ylabel('Correlation')\n",
    "                axes[idx].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Find peak correlation\n",
    "                peak_idx = np.argmax(np.abs(correlations))\n",
    "                peak_lag = lags[peak_idx]\n",
    "                peak_corr = correlations[peak_idx]\n",
    "                axes[idx].text(0.02, 0.95, f'Peak: {peak_corr:.3f} at lag {peak_lag}',\n",
    "                             transform=axes[idx].transAxes, verticalalignment='top',\n",
    "                             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Granger Causality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for Granger causality between key series\n",
    "if 'cycle' in components:\n",
    "    # Select a few key relationships to test\n",
    "    test_pairs = selected_pairs[:3]  # Limit to avoid too many tests\n",
    "    \n",
    "    granger_results = pd.DataFrame(columns=['Series1', 'Series2', 'Lag', 'F-statistic', 'p-value'])\n",
    "    \n",
    "    for series1, series2, label in test_pairs:\n",
    "        if series1 in cycle_df.columns and series2 in cycle_df.columns:\n",
    "            # Prepare data\n",
    "            data1 = cycle_df[series1].dropna()\n",
    "            data2 = cycle_df[series2].dropna()\n",
    "            common_idx = data1.index.intersection(data2.index)\n",
    "            \n",
    "            if len(common_idx) > 50:\n",
    "                test_data = pd.DataFrame({\n",
    "                    'x': data2.loc[common_idx],\n",
    "                    'y': data1.loc[common_idx]\n",
    "                })\n",
    "                \n",
    "                # Test for lags 1-4\n",
    "                print(f\"\\nGranger Causality Test: {label}\")\n",
    "                print(\"=\" * 50)\n",
    "                \n",
    "                for lag in range(1, 5):\n",
    "                    try:\n",
    "                        result = grangercausalitytests(test_data, maxlag=lag, verbose=False)\n",
    "                        f_stat = result[lag][0]['ssr_ftest'][0]\n",
    "                        p_value = result[lag][0]['ssr_ftest'][1]\n",
    "                        \n",
    "                        granger_results = pd.concat([granger_results, pd.DataFrame({\n",
    "                            'Series1': [series1],\n",
    "                            'Series2': [series2],\n",
    "                            'Lag': [lag],\n",
    "                            'F-statistic': [f_stat],\n",
    "                            'p-value': [p_value]\n",
    "                        })], ignore_index=True)\n",
    "                        \n",
    "                        if p_value < 0.05:\n",
    "                            print(f\"  Lag {lag}: F={f_stat:.3f}, p={p_value:.4f} ***\")\n",
    "                        else:\n",
    "                            print(f\"  Lag {lag}: F={f_stat:.3f}, p={p_value:.4f}\")\n",
    "                    except:\n",
    "                        print(f\"  Lag {lag}: Test failed\")\n",
    "    \n",
    "    # Display significant results\n",
    "    significant = granger_results[granger_results['p-value'] < 0.05]\n",
    "    if not significant.empty:\n",
    "        print(\"\\nSignificant Granger Causality Results (p < 0.05):\")\n",
    "        print(significant)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Velocity of Money Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize economic analyzer\n",
    "analyzer = EconomicAnalysis(components, z1_data)\n",
    "\n",
    "# Calculate velocity of money if relevant series are available\n",
    "# Look for GDP and M2 series\n",
    "gdp_series = [s for s in metadata['series'] if 'GDP' in s or 'FL90' in s]\n",
    "m2_series = [s for s in metadata['series'] if 'M2' in s or 'FL71' in s]\n",
    "\n",
    "if gdp_series and m2_series:\n",
    "    print(\"Calculating Velocity of Money...\")\n",
    "    velocity_results = analyzer.analyze_velocity_of_money(\n",
    "        gdp_series=gdp_series[0],\n",
    "        money_series=m2_series[0]\n",
    "    )\n",
    "    \n",
    "    if velocity_results:\n",
    "        # Plot velocity components\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "        \n",
    "        # Velocity level\n",
    "        axes[0].plot(velocity_results['velocity'].index, \n",
    "                    velocity_results['velocity'].values,\n",
    "                    linewidth=2)\n",
    "        axes[0].set_title('Velocity of Money (V = GDP/M2)')\n",
    "        axes[0].set_ylabel('Velocity')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Velocity trend\n",
    "        if 'velocity_trend' in velocity_results:\n",
    "            axes[1].plot(velocity_results['velocity_trend'].index,\n",
    "                        velocity_results['velocity_trend'].values,\n",
    "                        linewidth=2, color='orange')\n",
    "            axes[1].set_title('Velocity Trend Component')\n",
    "            axes[1].set_ylabel('Trend')\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Velocity cycle\n",
    "        if 'velocity_cycle' in velocity_results:\n",
    "            axes[2].plot(velocity_results['velocity_cycle'].index,\n",
    "                        velocity_results['velocity_cycle'].values,\n",
    "                        linewidth=2, color='green')\n",
    "            axes[2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "            axes[2].set_title('Velocity Cycle Component')\n",
    "            axes[2].set_ylabel('Cycle')\n",
    "            axes[2].set_xlabel('Date')\n",
    "            axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"GDP or M2 series not found for velocity calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sector Flow Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze flows between sectors using trend components\n",
    "if 'trend' in components:\n",
    "    trend_df = components['trend']\n",
    "    \n",
    "    # Create sector aggregates\n",
    "    sector_trends = {}\n",
    "    for sector, series_list in series_categories.items():\n",
    "        if series_list:\n",
    "            available_series = [s for s in series_list if s in trend_df.columns]\n",
    "            if available_series:\n",
    "                sector_trends[sector] = trend_df[available_series].mean(axis=1)\n",
    "    \n",
    "    if len(sector_trends) > 2:\n",
    "        # Plot sector trends\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        \n",
    "        for sector, trend in sector_trends.items():\n",
    "            ax.plot(trend.index, trend.values, linewidth=2, label=sector.capitalize())\n",
    "        \n",
    "        ax.set_title('Average Trend by Economic Sector')\n",
    "        ax.set_ylabel('Average Trend Level')\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.legend(loc='upper left')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Compute sector correlation matrix\n",
    "        sector_df = pd.DataFrame(sector_trends)\n",
    "        sector_corr = sector_df.corr()\n",
    "        \n",
    "        # Plot correlation heatmap\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.heatmap(sector_corr, annot=True, fmt='.3f', cmap='coolwarm',\n",
    "                   center=0, square=True, linewidths=1)\n",
    "        plt.title('Sector Trend Correlations')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dynamic Factor Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract common factors from cycle components using PCA\n",
    "if 'cycle' in components:\n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    # Prepare data\n",
    "    cycle_clean = cycle_df.dropna(axis=1, how='any')\n",
    "    \n",
    "    if cycle_clean.shape[1] > 5:\n",
    "        # Standardize data\n",
    "        scaler = StandardScaler()\n",
    "        cycle_scaled = scaler.fit_transform(cycle_clean)\n",
    "        \n",
    "        # Perform PCA\n",
    "        n_components = min(10, cycle_clean.shape[1])\n",
    "        pca = PCA(n_components=n_components)\n",
    "        factors = pca.fit_transform(cycle_scaled)\n",
    "        \n",
    "        # Create factor DataFrame\n",
    "        factor_df = pd.DataFrame(\n",
    "            factors,\n",
    "            index=cycle_clean.index,\n",
    "            columns=[f'Factor{i+1}' for i in range(n_components)]\n",
    "        )\n",
    "        \n",
    "        # Plot explained variance\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Scree plot\n",
    "        ax1.bar(range(1, n_components+1), pca.explained_variance_ratio_)\n",
    "        ax1.set_xlabel('Component')\n",
    "        ax1.set_ylabel('Explained Variance Ratio')\n",
    "        ax1.set_title('PCA Scree Plot')\n",
    "        ax1.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Cumulative variance\n",
    "        cumvar = np.cumsum(pca.explained_variance_ratio_)\n",
    "        ax2.plot(range(1, n_components+1), cumvar, 'o-', linewidth=2)\n",
    "        ax2.axhline(y=0.8, color='red', linestyle='--', alpha=0.5)\n",
    "        ax2.set_xlabel('Number of Components')\n",
    "        ax2.set_ylabel('Cumulative Explained Variance')\n",
    "        ax2.set_title('Cumulative Variance Explained')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot first three factors\n",
    "        fig, axes = plt.subplots(3, 1, figsize=(12, 10))\n",
    "        \n",
    "        for i in range(min(3, n_components)):\n",
    "            axes[i].plot(factor_df.index, factor_df[f'Factor{i+1}'], linewidth=1.5)\n",
    "            axes[i].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "            axes[i].set_title(f'Factor {i+1} (explains {pca.explained_variance_ratio_[i]:.1%} of variance)')\n",
    "            axes[i].set_ylabel('Factor Value')\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        axes[-1].set_xlabel('Date')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nTotal variance explained by {n_components} factors: {cumvar[-1]:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Recession Indicator Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze how components behave around recessions\n",
    "# Load NBER recession dates\n",
    "recession_dates = [\n",
    "    ('1990-07-01', '1991-03-31'),\n",
    "    ('2001-03-01', '2001-11-30'),\n",
    "    ('2007-12-01', '2009-06-30'),\n",
    "    ('2020-02-01', '2020-04-30')\n",
    "]\n",
    "\n",
    "# Convert to datetime\n",
    "recession_periods = [(pd.to_datetime(start), pd.to_datetime(end)) \n",
    "                    for start, end in recession_dates]\n",
    "\n",
    "# Analyze cycle behavior around recessions\n",
    "if 'cycle' in components and len(cycle_clean.columns) > 0:\n",
    "    # Calculate average cycle values\n",
    "    avg_cycle = cycle_clean.mean(axis=1)\n",
    "    \n",
    "    # Plot with recession shading\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Plot average cycle\n",
    "    ax.plot(avg_cycle.index, avg_cycle.values, linewidth=2, label='Average Cycle')\n",
    "    \n",
    "    # Add recession shading\n",
    "    for start, end in recession_periods:\n",
    "        if start >= avg_cycle.index.min() and end <= avg_cycle.index.max():\n",
    "            ax.axvspan(start, end, alpha=0.3, color='gray', label='Recession' if start == recession_periods[0][0] else '')\n",
    "    \n",
    "    ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "    ax.set_title('Average Economic Cycle Component with Recession Periods')\n",
    "    ax.set_ylabel('Average Cycle Value')\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate cycle statistics during recessions vs expansions\n",
    "    recession_mask = pd.Series(False, index=avg_cycle.index)\n",
    "    for start, end in recession_periods:\n",
    "        mask = (avg_cycle.index >= start) & (avg_cycle.index <= end)\n",
    "        recession_mask = recession_mask | mask\n",
    "    \n",
    "    recession_stats = avg_cycle[recession_mask].describe()\n",
    "    expansion_stats = avg_cycle[~recession_mask].describe()\n",
    "    \n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Recession': recession_stats,\n",
    "        'Expansion': expansion_stats\n",
    "    })\n",
    "    \n",
    "    print(\"Cycle Component Statistics:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(comparison_df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Network Visualization of Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a network visualization of strong relationships\n",
    "import networkx as nx\n",
    "\n",
    "if 'cycle' in components:\n",
    "    # Compute correlation matrix for cycle components\n",
    "    cycle_corr = cycle_clean.corr()\n",
    "    \n",
    "    # Create network from strong correlations\n",
    "    threshold = 0.7  # Only show strong correlations\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add nodes\n",
    "    for series in cycle_corr.columns:\n",
    "        # Determine node category\n",
    "        node_category = 'other'\n",
    "        for category, series_list in series_categories.items():\n",
    "            if series in series_list:\n",
    "                node_category = category\n",
    "                break\n",
    "        G.add_node(series, category=node_category)\n",
    "    \n",
    "    # Add edges for strong correlations\n",
    "    for i in range(len(cycle_corr.columns)):\n",
    "        for j in range(i+1, len(cycle_corr.columns)):\n",
    "            corr_value = cycle_corr.iloc[i, j]\n",
    "            if abs(corr_value) > threshold:\n",
    "                G.add_edge(cycle_corr.columns[i], cycle_corr.columns[j], \n",
    "                          weight=abs(corr_value), \n",
    "                          correlation=corr_value)\n",
    "    \n",
    "    if G.number_of_edges() > 0:\n",
    "        # Create layout\n",
    "        pos = nx.spring_layout(G, k=2, iterations=50)\n",
    "        \n",
    "        # Set up colors by category\n",
    "        color_map = {\n",
    "            'household': 'lightblue',\n",
    "            'corporate': 'lightgreen',\n",
    "            'financial': 'lightcoral',\n",
    "            'government': 'lightyellow',\n",
    "            'foreign': 'lightpink',\n",
    "            'monetary': 'lightgray',\n",
    "            'other': 'white'\n",
    "        }\n",
    "        \n",
    "        node_colors = [color_map.get(G.nodes[node].get('category', 'other'), 'white') \n",
    "                      for node in G.nodes()]\n",
    "        \n",
    "        # Draw network\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        # Draw nodes\n",
    "        nx.draw_networkx_nodes(G, pos, node_color=node_colors, \n",
    "                              node_size=500, alpha=0.8)\n",
    "        \n",
    "        # Draw edges with varying width based on correlation strength\n",
    "        edges = G.edges()\n",
    "        weights = [G[u][v]['weight'] * 3 for u, v in edges]\n",
    "        edge_colors = ['red' if G[u][v]['correlation'] < 0 else 'blue' for u, v in edges]\n",
    "        \n",
    "        nx.draw_networkx_edges(G, pos, width=weights, alpha=0.5, edge_color=edge_colors)\n",
    "        \n",
    "        # Draw labels\n",
    "        labels = {node: node.split('.')[-1][:10] for node in G.nodes()}\n",
    "        nx.draw_networkx_labels(G, pos, labels, font_size=8)\n",
    "        \n",
    "        plt.title(f'Network of Strong Correlations (|r| > {threshold})')\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Add legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [Patch(facecolor=color, label=cat.capitalize()) \n",
    "                          for cat, color in color_map.items() if cat != 'other']\n",
    "        plt.legend(handles=legend_elements, loc='upper right')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Network has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "        print(f\"Network density: {nx.density(G):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Save Analysis Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save key analysis results\n",
    "output_dir = Path('../data/economic_analysis')\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save Granger causality results\n",
    "if 'granger_results' in locals() and not granger_results.empty:\n",
    "    granger_results.to_csv(output_dir / 'granger_causality_results.csv', index=False)\n",
    "    print(\"Saved Granger causality results\")\n",
    "\n",
    "# Save sector correlations\n",
    "if 'sector_corr' in locals():\n",
    "    sector_corr.to_csv(output_dir / 'sector_correlations.csv')\n",
    "    print(\"Saved sector correlations\")\n",
    "\n",
    "# Save PCA results\n",
    "if 'factor_df' in locals():\n",
    "    factor_df.to_parquet(output_dir / 'economic_factors.parquet')\n",
    "    \n",
    "    # Save loadings\n",
    "    loadings_df = pd.DataFrame(\n",
    "        pca.components_[:3].T,\n",
    "        index=cycle_clean.columns,\n",
    "        columns=['PC1', 'PC2', 'PC3']\n",
    "    )\n",
    "    loadings_df.to_csv(output_dir / 'factor_loadings.csv')\n",
    "    print(\"Saved PCA factors and loadings\")\n",
    "\n",
    "# Save network data\n",
    "if 'G' in locals() and G.number_of_edges() > 0:\n",
    "    nx.write_gexf(G, output_dir / 'correlation_network.gexf')\n",
    "    print(\"Saved correlation network\")\n",
    "\n",
    "print(\"\\n Economic relationships analysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
